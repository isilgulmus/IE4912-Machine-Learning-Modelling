{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46f7d1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f1715ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(path):\n",
    "    return pd.read_csv(path)\n",
    "load_dotenv() \n",
    "data_path = os.getenv(\"TRAINING_DATA\")\n",
    "df = read_data(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31a5b8d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((154851, 31), (38713, 31), (154851,), (38713,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Split the dataset into features and target, then divide it into training and testing sets.\n",
    "\n",
    "- X: feature matrix (all columns except the target)\n",
    "- y: target variable ('HATSURESI')\n",
    "- 90% of the data is used for training, 10% for testing\n",
    "- The random_state ensures reproducible results\n",
    "\"\"\"\n",
    "\n",
    "# Separate features (X) and target variable (y)\n",
    "X = df.drop([\"HATSURESI\",\"Unnamed: 0\", \"Unnamed: 0.1\"], axis=1)  # Drop target column to create feature set\n",
    "y = df[\"HATSURESI\"]                 # Target variable to predict\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Display the shape of the resulting splits\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc7e6ed",
   "metadata": {},
   "source": [
    "### Model Stacking – XGBoost + CatBoost + LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7a919250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔁 Model Stacking Results:\n",
      "R² Score: 0.7013\n",
      "RMSE: 6.43\n",
      "MAE: 4.23\n",
      "MAPE: 7.00%\n"
     ]
    }
   ],
   "source": [
    "# Define a list of base models to be used in stacking\n",
    "base_models = [\n",
    "    ('xgb', XGBRegressor(random_state=42)),                # XGBoost Regressor\n",
    "    ('cat', CatBoostRegressor(verbose=0, random_state=42)),# CatBoost Regressor\n",
    "    ('lr', LinearRegression())                             # Linear Regression\n",
    "]\n",
    "\n",
    "\"\"\"\n",
    "Create a stacking model that combines predictions from multiple base models using a meta-learner.\n",
    "\n",
    "- base_models: XGBoost, CatBoost, and Linear Regression\n",
    "- final_estimator: Linear Regression used to learn from base model outputs\n",
    "- n_jobs=-1: uses all available CPU cores for parallel processing\n",
    "\"\"\"\n",
    "stacked_model = StackingRegressor(\n",
    "    estimators=base_models,\n",
    "    final_estimator=LinearRegression(),  # Meta-learner\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "Train the stacked model on the training dataset.\n",
    "\n",
    "- X_train: training features\n",
    "- y_train: training target values\n",
    "\"\"\"\n",
    "stacked_model.fit(X_train, y_train)\n",
    "\n",
    "\"\"\"\n",
    "Make predictions on the test set using the trained stacked model.\n",
    "\n",
    "- X_test: test features\n",
    "- y_pred_stack: predicted values from the stacked model\n",
    "\"\"\"\n",
    "y_pred_stack = stacked_model.predict(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Evaluate the performance of the stacked model using various metrics:\n",
    "\"\"\"\n",
    "r2 = r2_score(y_test, y_pred_stack)\n",
    "mse = mean_squared_error(y_test, y_pred_stack)\n",
    "rmse = mse ** 0.5 \n",
    "mae = mean_absolute_error(y_test, y_pred_stack)\n",
    "mape = np.mean(np.abs((y_test - y_pred_stack) / y_test)) * 100\n",
    "\n",
    "\"\"\"\n",
    "Print the evaluation results of the stacked model.\n",
    "\"\"\"\n",
    "print(\"\\n🔁 Model Stacking Results:\")\n",
    "print(f\"R² Score: {r2:.4f}\")\n",
    "print(f\"RMSE: {rmse:.2f}\")\n",
    "print(f\"MAE: {mae:.2f}\")\n",
    "print(f\"MAPE: {mape:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd34ed1",
   "metadata": {},
   "source": [
    "#### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e14604f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 192 candidates, totalling 576 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Excalibur\\Desktop\\Bitirme Projesi\\Datas\\800-last\\IE-4912\\venv\\Lib\\site-packages\\joblib\\parallel.py:1682\u001b[39m, in \u001b[36mParallel._get_outputs\u001b[39m\u001b[34m(self, iterator, pre_dispatch)\u001b[39m\n\u001b[32m   1681\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backend.retrieval_context():\n\u001b[32m-> \u001b[39m\u001b[32m1682\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retrieve()\n\u001b[32m   1684\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[32m   1685\u001b[39m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[32m   1686\u001b[39m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[32m   1687\u001b[39m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Excalibur\\Desktop\\Bitirme Projesi\\Datas\\800-last\\IE-4912\\venv\\Lib\\site-packages\\joblib\\parallel.py:1800\u001b[39m, in \u001b[36mParallel._retrieve\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1797\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (nb_jobs == \u001b[32m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   1798\u001b[39m     \u001b[38;5;28mself\u001b[39m._jobs[\u001b[32m0\u001b[39m].get_status(timeout=\u001b[38;5;28mself\u001b[39m.timeout) == TASK_PENDING\n\u001b[32m   1799\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1800\u001b[39m     \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1801\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 68\u001b[39m\n\u001b[32m     55\u001b[39m grid_search = GridSearchCV(\n\u001b[32m     56\u001b[39m     estimator=stacked_model,\n\u001b[32m     57\u001b[39m     param_grid=param_grid,\n\u001b[32m   (...)\u001b[39m\u001b[32m     61\u001b[39m     n_jobs=-\u001b[32m1\u001b[39m\n\u001b[32m     62\u001b[39m )\n\u001b[32m     64\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     65\u001b[39m \u001b[33;03mTrain the grid search on the training data.\u001b[39;00m\n\u001b[32m     66\u001b[39m \n\u001b[32m     67\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m \u001b[43mgrid_search\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[33;03mMake predictions using the best model found during grid search.\u001b[39;00m\n\u001b[32m     72\u001b[39m \n\u001b[32m     73\u001b[39m \u001b[33;03m- best_stack: the best performing model (with optimal parameters)\u001b[39;00m\n\u001b[32m     74\u001b[39m \u001b[33;03m- y_pred_stack: predicted values for the test set\u001b[39;00m\n\u001b[32m     75\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     76\u001b[39m best_stack = grid_search.best_estimator_\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Excalibur\\Desktop\\Bitirme Projesi\\Datas\\800-last\\IE-4912\\venv\\Lib\\site-packages\\sklearn\\base.py:1363\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1356\u001b[39m     estimator._validate_params()\n\u001b[32m   1358\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1359\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1360\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1361\u001b[39m     )\n\u001b[32m   1362\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1363\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Excalibur\\Desktop\\Bitirme Projesi\\Datas\\800-last\\IE-4912\\venv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1051\u001b[39m, in \u001b[36mBaseSearchCV.fit\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m   1045\u001b[39m     results = \u001b[38;5;28mself\u001b[39m._format_results(\n\u001b[32m   1046\u001b[39m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[32m   1047\u001b[39m     )\n\u001b[32m   1049\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[32m-> \u001b[39m\u001b[32m1051\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1053\u001b[39m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[32m   1054\u001b[39m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[32m   1055\u001b[39m first_test_score = all_out[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mtest_scores\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Excalibur\\Desktop\\Bitirme Projesi\\Datas\\800-last\\IE-4912\\venv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1605\u001b[39m, in \u001b[36mGridSearchCV._run_search\u001b[39m\u001b[34m(self, evaluate_candidates)\u001b[39m\n\u001b[32m   1603\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[32m   1604\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1605\u001b[39m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Excalibur\\Desktop\\Bitirme Projesi\\Datas\\800-last\\IE-4912\\venv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:997\u001b[39m, in \u001b[36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[39m\u001b[34m(candidate_params, cv, more_results)\u001b[39m\n\u001b[32m    989\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.verbose > \u001b[32m0\u001b[39m:\n\u001b[32m    990\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[32m    991\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[33m candidates,\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    992\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m fits\u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m    993\u001b[39m             n_splits, n_candidates, n_candidates * n_splits\n\u001b[32m    994\u001b[39m         )\n\u001b[32m    995\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m997\u001b[39m out = \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    998\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    999\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1000\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1001\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1002\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1003\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1004\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1005\u001b[39m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1006\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1007\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1008\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1009\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1010\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1011\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplitter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1012\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1013\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1015\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) < \u001b[32m1\u001b[39m:\n\u001b[32m   1016\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1017\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mNo fits were performed. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1018\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWas the CV iterator empty? \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1019\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWere there no candidates?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1020\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Excalibur\\Desktop\\Bitirme Projesi\\Datas\\800-last\\IE-4912\\venv\\Lib\\site-packages\\sklearn\\utils\\parallel.py:82\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     73\u001b[39m warning_filters = warnings.filters\n\u001b[32m     74\u001b[39m iterable_with_config_and_warning_filters = (\n\u001b[32m     75\u001b[39m     (\n\u001b[32m     76\u001b[39m         _with_config_and_warning_filters(delayed_func, config, warning_filters),\n\u001b[32m   (...)\u001b[39m\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     81\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config_and_warning_filters\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Excalibur\\Desktop\\Bitirme Projesi\\Datas\\800-last\\IE-4912\\venv\\Lib\\site-packages\\joblib\\parallel.py:2072\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   2066\u001b[39m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[32m   2067\u001b[39m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[32m   2068\u001b[39m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[32m   2069\u001b[39m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[32m   2070\u001b[39m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m2072\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Excalibur\\Desktop\\Bitirme Projesi\\Datas\\800-last\\IE-4912\\venv\\Lib\\site-packages\\joblib\\parallel.py:1732\u001b[39m, in \u001b[36mParallel._get_outputs\u001b[39m\u001b[34m(self, iterator, pre_dispatch)\u001b[39m\n\u001b[32m   1730\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:\n\u001b[32m   1731\u001b[39m     \u001b[38;5;28mself\u001b[39m._exception = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1732\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_abort\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1733\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m   1734\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m   1735\u001b[39m     \u001b[38;5;66;03m# Store the unconsumed tasks and terminate the workers if necessary\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Excalibur\\Desktop\\Bitirme Projesi\\Datas\\800-last\\IE-4912\\venv\\Lib\\site-packages\\joblib\\parallel.py:1646\u001b[39m, in \u001b[36mParallel._abort\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1641\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._aborted \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(backend, \u001b[33m\"\u001b[39m\u001b[33mabort_everything\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1642\u001b[39m     \u001b[38;5;66;03m# If the backend is managed externally we need to make sure\u001b[39;00m\n\u001b[32m   1643\u001b[39m     \u001b[38;5;66;03m# to leave it in a working state to allow for future jobs\u001b[39;00m\n\u001b[32m   1644\u001b[39m     \u001b[38;5;66;03m# scheduling.\u001b[39;00m\n\u001b[32m   1645\u001b[39m     ensure_ready = \u001b[38;5;28mself\u001b[39m._managed_backend\n\u001b[32m-> \u001b[39m\u001b[32m1646\u001b[39m     \u001b[43mbackend\u001b[49m\u001b[43m.\u001b[49m\u001b[43mabort_everything\u001b[49m\u001b[43m(\u001b[49m\u001b[43mensure_ready\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_ready\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1647\u001b[39m \u001b[38;5;28mself\u001b[39m._aborted = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Excalibur\\Desktop\\Bitirme Projesi\\Datas\\800-last\\IE-4912\\venv\\Lib\\site-packages\\joblib\\_parallel_backends.py:725\u001b[39m, in \u001b[36mLokyBackend.abort_everything\u001b[39m\u001b[34m(self, ensure_ready)\u001b[39m\n\u001b[32m    723\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mabort_everything\u001b[39m(\u001b[38;5;28mself\u001b[39m, ensure_ready=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m    724\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Shutdown the workers and restart a new one with the same parameters\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m725\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_workers\u001b[49m\u001b[43m.\u001b[49m\u001b[43mterminate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkill_workers\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    726\u001b[39m     \u001b[38;5;28mself\u001b[39m._workers = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    728\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ensure_ready:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Excalibur\\Desktop\\Bitirme Projesi\\Datas\\800-last\\IE-4912\\venv\\Lib\\site-packages\\joblib\\executor.py:86\u001b[39m, in \u001b[36mMemmappingExecutor.terminate\u001b[39m\u001b[34m(self, kill_workers)\u001b[39m\n\u001b[32m     85\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mterminate\u001b[39m(\u001b[38;5;28mself\u001b[39m, kill_workers=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mshutdown\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkill_workers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkill_workers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     88\u001b[39m     \u001b[38;5;66;03m# When workers are killed in a brutal manner, they cannot execute the\u001b[39;00m\n\u001b[32m     89\u001b[39m     \u001b[38;5;66;03m# finalizer of their shared memmaps. The refcount of those memmaps may\u001b[39;00m\n\u001b[32m     90\u001b[39m     \u001b[38;5;66;03m# be off by an unknown number, so instead of decref'ing them, we force\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     95\u001b[39m     \u001b[38;5;66;03m# with allow_non_empty=True but if we can't, it will be clean up later\u001b[39;00m\n\u001b[32m     96\u001b[39m     \u001b[38;5;66;03m# on by the resource_tracker.\u001b[39;00m\n\u001b[32m     97\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._submit_resize_lock:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Excalibur\\Desktop\\Bitirme Projesi\\Datas\\800-last\\IE-4912\\venv\\Lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:1333\u001b[39m, in \u001b[36mProcessPoolExecutor.shutdown\u001b[39m\u001b[34m(self, wait, kill_workers)\u001b[39m\n\u001b[32m   1329\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m executor_manager_thread \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m wait:\n\u001b[32m   1330\u001b[39m     \u001b[38;5;66;03m# This locks avoids concurrent join if the interpreter\u001b[39;00m\n\u001b[32m   1331\u001b[39m     \u001b[38;5;66;03m# is shutting down.\u001b[39;00m\n\u001b[32m   1332\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m _global_shutdown_lock:\n\u001b[32m-> \u001b[39m\u001b[32m1333\u001b[39m         \u001b[43mexecutor_manager_thread\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1334\u001b[39m         _threads_wakeups.pop(executor_manager_thread, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m   1336\u001b[39m \u001b[38;5;66;03m# To reduce the risk of opening too many files, remove references to\u001b[39;00m\n\u001b[32m   1337\u001b[39m \u001b[38;5;66;03m# objects that use file descriptors.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\threading.py:1092\u001b[39m, in \u001b[36mThread.join\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   1089\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1090\u001b[39m     timeout = \u001b[38;5;28mmax\u001b[39m(timeout, \u001b[32m0\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1092\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_handle\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "xgb_model = XGBRegressor(random_state=42)\n",
    "cat_model = CatBoostRegressor(verbose=0, random_state=42)\n",
    "\n",
    "\n",
    "base_models = [\n",
    "    ('xgb', xgb_model),\n",
    "    ('cat', cat_model)\n",
    "]\n",
    "\n",
    "\n",
    "final_estimator = Ridge()\n",
    "\n",
    "\n",
    "stacked_model = StackingRegressor(\n",
    "    estimators=base_models,\n",
    "    final_estimator=final_estimator,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Define a parameter grid for hyperparameter tuning using GridSearchCV.\n",
    "\n",
    "- Includes parameters for:\n",
    "  - XGBoost: number of estimators, max depth, learning rate\n",
    "  - CatBoost: number of iterations, depth, learning rate\n",
    "  - Final estimator (Ridge Regression): regularization strength (alpha)\n",
    "- The double underscores (e.g., xgb__n_estimators) refer to parameters of sub-models in the pipeline\n",
    "\"\"\"\n",
    "param_grid = {\n",
    "    # XGBoost parameters\n",
    "    'xgb__n_estimators': [100, 300],\n",
    "    'xgb__max_depth': [3, 5],\n",
    "    'xgb__learning_rate': [0.05, 0.1],\n",
    "    \n",
    "    # CatBoost parameters\n",
    "    'cat__iterations': [200, 300],\n",
    "    'cat__depth': [6, 8],\n",
    "    'cat__learning_rate': [0.05, 0.1],\n",
    "\n",
    "    # Final estimator (Ridge Regression) parameter\n",
    "    'final_estimator__alpha': [0.1, 1.0, 10.0]\n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "Perform a grid search to find the best combination of hyperparameters for the stacked model.\n",
    "\n",
    "- estimator: the stacking model that includes base and final estimators\n",
    "- param_grid: dictionary containing all combinations of parameters to test\n",
    "- cv=3: 3-fold cross-validation is used for evaluation\n",
    "- scoring='r2': R² score is used as the evaluation metric\n",
    "- verbose=2: prints detailed log output during training\n",
    "- n_jobs=-1: uses all available CPU cores\n",
    "\"\"\"\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=stacked_model,\n",
    "    param_grid=param_grid,\n",
    "    cv=3,\n",
    "    scoring='r2',\n",
    "    verbose=2,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "Train the grid search on the training data.\n",
    "\n",
    "\"\"\"\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "\"\"\"\n",
    "Make predictions using the best model found during grid search.\n",
    "\n",
    "- best_stack: the best performing model (with optimal parameters)\n",
    "- y_pred_stack: predicted values for the test set\n",
    "\"\"\"\n",
    "best_stack = grid_search.best_estimator_\n",
    "y_pred_stack = best_stack.predict(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Evaluate the optimized model's performance on the test set.\n",
    "\n",
    "\"\"\"\n",
    "r2 = r2_score(y_test, y_pred_stack)\n",
    "mse = mean_squared_error(y_test, y_pred_stack)\n",
    "rmse = mse ** 0.5 \n",
    "mae = mean_absolute_error(y_test, y_pred_stack)\n",
    "mape = np.mean(np.abs((y_test - y_pred_stack) / y_test.replace(0, 1e-10))) * 100\n",
    "\n",
    "print(\"\\n🔁 Optimized Full Stacking Model Results:\")\n",
    "print(\"🔧 Best Parameters:\", grid_search.best_params_)\n",
    "print(f\"R² Score: {r2:.4f}\")\n",
    "print(f\"RMSE: {rmse:.2f}\")\n",
    "print(f\"MAE: {mae:.2f}\")\n",
    "print(f\"MAPE: {mape:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc15d2e",
   "metadata": {},
   "source": [
    "### Model Stacking – XGBoost + CatBoost + Ridge & Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ab520ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📘 Ridge Final Estimator:\n",
      "R² Score: 0.7011\n",
      "RMSE: 6.43\n",
      "MAE: 4.23\n",
      "MAPE: 7.00%\n",
      "\n",
      "📙 Lasso Final Estimator:\n",
      "R² Score: 0.7011\n",
      "RMSE: 6.43\n",
      "MAE: 4.23\n",
      "MAPE: 7.00%\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Define base models for stacking.\n",
    "\n",
    "- base_models: a list of tuples containing model names and their corresponding regressors\n",
    "- XGBRegressor and CatBoostRegressor are used with default parameters\n",
    "- random_state ensures reproducibility\n",
    "\"\"\"\n",
    "base_models = [\n",
    "    ('xgb', XGBRegressor(random_state=42)),\n",
    "    ('cat', CatBoostRegressor(verbose=0, random_state=42)),\n",
    "]\n",
    "\n",
    "\"\"\"\n",
    "Define two different final estimators (meta-learners) to compare:\n",
    "\n",
    "- Ridge: L2-regularized linear regression\n",
    "- Lasso: L1-regularized linear regression\n",
    "\"\"\"\n",
    "ridge = Ridge(alpha=1.0)\n",
    "lasso = Lasso(alpha=0.1)\n",
    "\n",
    "\"\"\"\n",
    "Create and train a stacking model using Ridge as the final estimator.\n",
    "\n",
    "- estimators: base models (XGB + CatBoost)\n",
    "- final_estimator: Ridge Regression\n",
    "- n_jobs=-1: utilizes all CPU cores for parallel processing\n",
    "\"\"\"\n",
    "stack_ridge = StackingRegressor(estimators=base_models, final_estimator=ridge, n_jobs=-1)\n",
    "stack_ridge.fit(X_train, y_train)\n",
    "\n",
    "\"\"\"\n",
    "Generate predictions using the Ridge-based stacked model.\n",
    "\n",
    "- y_pred_ridge: predictions on the test set\n",
    "\"\"\"\n",
    "y_pred_ridge = stack_ridge.predict(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Create and train a stacking model using Lasso as the final estimator.\n",
    "\n",
    "- final_estimator: Lasso Regression (L1 regularization)\n",
    "\"\"\"\n",
    "stack_lasso = StackingRegressor(estimators=base_models, final_estimator=lasso, n_jobs=-1)\n",
    "stack_lasso.fit(X_train, y_train)\n",
    "\n",
    "\"\"\"\n",
    "Generate predictions using the Lasso-based stacked model.\n",
    "\n",
    "- y_pred_lasso: predictions on the test set\n",
    "\"\"\"\n",
    "y_pred_lasso = stack_lasso.predict(X_test)\n",
    "\n",
    "def evaluate_model(name, y_pred):\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = mse ** 0.5 \n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100  # handles MAPE calculation\n",
    "\n",
    "    print(f\"\\n{name} Final Estimator:\")\n",
    "    print(f\"R² Score: {r2:.4f}\")\n",
    "    print(f\"RMSE: {rmse:.2f}\")\n",
    "    print(f\"MAE: {mae:.2f}\")\n",
    "    print(f\"MAPE: {mape:.2f}%\")\n",
    "\n",
    "\"\"\"\n",
    "Evaluate and compare both stacking models (Ridge vs. Lasso).\n",
    "\"\"\"\n",
    "evaluate_model(\"📘 Ridge\", y_pred_ridge)\n",
    "evaluate_model(\"📙 Lasso\", y_pred_lasso)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
