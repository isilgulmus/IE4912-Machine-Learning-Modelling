{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f7d1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1715ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(path):\n",
    "    return pd.read_csv(path)\n",
    "load_dotenv() \n",
    "data_path = os.getenv(\"TRAINING_DATA\")\n",
    "df = read_data(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a5b8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Split the dataset into features and target, then divide it into training and testing sets.\n",
    "\n",
    "- X: feature matrix (all columns except the target)\n",
    "- y: target variable ('HATSURESI')\n",
    "- 90% of the data is used for training, 10% for testing\n",
    "- The random_state ensures reproducible results\n",
    "\"\"\"\n",
    "\n",
    "# Separate features (X) and target variable (y)\n",
    "X = df.drop([\"HATSURESI\",\"Unnamed: 0\", \"Unnamed: 0.1\"], axis=1)  # Drop target column to create feature set\n",
    "y = df[\"HATSURESI\"]                 # Target variable to predict\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Display the shape of the resulting splits\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc7e6ed",
   "metadata": {},
   "source": [
    "### Model Stacking ‚Äì XGBoost + CatBoost + LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a919250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of base models to be used in stacking\n",
    "base_models = [\n",
    "    ('xgb', XGBRegressor(random_state=42)),                # XGBoost Regressor\n",
    "    ('cat', CatBoostRegressor(verbose=0, random_state=42)),# CatBoost Regressor\n",
    "    ('lr', LinearRegression())                             # Linear Regression\n",
    "]\n",
    "\n",
    "\"\"\"\n",
    "Create a stacking model that combines predictions from multiple base models using a meta-learner.\n",
    "\n",
    "- base_models: XGBoost, CatBoost, and Linear Regression\n",
    "- final_estimator: Linear Regression used to learn from base model outputs\n",
    "- n_jobs=-1: uses all available CPU cores for parallel processing\n",
    "\"\"\"\n",
    "stacked_model = StackingRegressor(\n",
    "    estimators=base_models,\n",
    "    final_estimator=LinearRegression(),  # Meta-learner\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "Train the stacked model on the training dataset.\n",
    "\n",
    "- X_train: training features\n",
    "- y_train: training target values\n",
    "\"\"\"\n",
    "stacked_model.fit(X_train, y_train)\n",
    "\n",
    "\"\"\"\n",
    "Make predictions on the test set using the trained stacked model.\n",
    "\n",
    "- X_test: test features\n",
    "- y_pred_stack: predicted values from the stacked model\n",
    "\"\"\"\n",
    "y_pred_stack = stacked_model.predict(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Evaluate the performance of the stacked model using various metrics:\n",
    "\"\"\"\n",
    "r2 = r2_score(y_test, y_pred_stack)\n",
    "mse = mean_squared_error(y_test, y_pred_stack)\n",
    "rmse = mse ** 0.5 \n",
    "mae = mean_absolute_error(y_test, y_pred_stack)\n",
    "mape = np.mean(np.abs((y_test - y_pred_stack) / y_test)) * 100\n",
    "\n",
    "\"\"\"\n",
    "Print the evaluation results of the stacked model.\n",
    "\"\"\"\n",
    "print(\"\\nüîÅ Model Stacking Results:\")\n",
    "print(f\"R¬≤ Score: {r2:.4f}\")\n",
    "print(f\"RMSE: {rmse:.2f}\")\n",
    "print(f\"MAE: {mae:.2f}\")\n",
    "print(f\"MAPE: {mape:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd34ed1",
   "metadata": {},
   "source": [
    "#### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14604f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = XGBRegressor(random_state=42)\n",
    "cat_model = CatBoostRegressor(verbose=0, random_state=42)\n",
    "\n",
    "\n",
    "base_models = [\n",
    "    ('xgb', xgb_model),\n",
    "    ('cat', cat_model)\n",
    "]\n",
    "\n",
    "\n",
    "final_estimator = Ridge()\n",
    "\n",
    "\n",
    "stacked_model = StackingRegressor(\n",
    "    estimators=base_models,\n",
    "    final_estimator=final_estimator,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Define a parameter grid for hyperparameter tuning using GridSearchCV.\n",
    "\n",
    "- Includes parameters for:\n",
    "  - XGBoost: number of estimators, max depth, learning rate\n",
    "  - CatBoost: number of iterations, depth, learning rate\n",
    "  - Final estimator (Ridge Regression): regularization strength (alpha)\n",
    "- The double underscores (e.g., xgb__n_estimators) refer to parameters of sub-models in the pipeline\n",
    "\"\"\"\n",
    "param_grid = {\n",
    "    # XGBoost parameters\n",
    "    'xgb__n_estimators': [100, 300],\n",
    "    'xgb__max_depth': [3, 5],\n",
    "    'xgb__learning_rate': [0.05, 0.1],\n",
    "    \n",
    "    # CatBoost parameters\n",
    "    'cat__iterations': [200, 300],\n",
    "    'cat__depth': [6, 8],\n",
    "    'cat__learning_rate': [0.05, 0.1],\n",
    "\n",
    "    # Final estimator (Ridge Regression) parameter\n",
    "    'final_estimator__alpha': [0.1, 1.0, 10.0]\n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "Perform a grid search to find the best combination of hyperparameters for the stacked model.\n",
    "\n",
    "- estimator: the stacking model that includes base and final estimators\n",
    "- param_grid: dictionary containing all combinations of parameters to test\n",
    "- cv=3: 3-fold cross-validation is used for evaluation\n",
    "- scoring='r2': R¬≤ score is used as the evaluation metric\n",
    "- verbose=2: prints detailed log output during training\n",
    "- n_jobs=-1: uses all available CPU cores\n",
    "\"\"\"\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=stacked_model,\n",
    "    param_grid=param_grid,\n",
    "    cv=3,\n",
    "    scoring='r2',\n",
    "    verbose=2,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "Train the grid search on the training data.\n",
    "\n",
    "\"\"\"\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "\"\"\"\n",
    "Make predictions using the best model found during grid search.\n",
    "\n",
    "- best_stack: the best performing model (with optimal parameters)\n",
    "- y_pred_stack: predicted values for the test set\n",
    "\"\"\"\n",
    "best_stack = grid_search.best_estimator_\n",
    "y_pred_stack = best_stack.predict(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Evaluate the optimized model's performance on the test set.\n",
    "\n",
    "\"\"\"\n",
    "r2 = r2_score(y_test, y_pred_stack)\n",
    "mse = mean_squared_error(y_test, y_pred_stack)\n",
    "rmse = mse ** 0.5 \n",
    "mae = mean_absolute_error(y_test, y_pred_stack)\n",
    "mape = np.mean(np.abs((y_test - y_pred_stack) / y_test.replace(0, 1e-10))) * 100\n",
    "\n",
    "print(\"\\nüîÅ Optimized Full Stacking Model Results:\")\n",
    "print(\"üîß Best Parameters:\", grid_search.best_params_)\n",
    "print(f\"R¬≤ Score: {r2:.4f}\")\n",
    "print(f\"RMSE: {rmse:.2f}\")\n",
    "print(f\"MAE: {mae:.2f}\")\n",
    "print(f\"MAPE: {mape:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc15d2e",
   "metadata": {},
   "source": [
    "### Model Stacking ‚Äì XGBoost + CatBoost + Ridge & Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab520ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Define base models for stacking.\n",
    "\n",
    "- base_models: a list of tuples containing model names and their corresponding regressors\n",
    "- XGBRegressor and CatBoostRegressor are used with default parameters\n",
    "- random_state ensures reproducibility\n",
    "\"\"\"\n",
    "base_models = [\n",
    "    ('xgb', XGBRegressor(random_state=42)),\n",
    "    ('cat', CatBoostRegressor(verbose=0, random_state=42)),\n",
    "]\n",
    "\n",
    "\"\"\"\n",
    "Define two different final estimators (meta-learners) to compare:\n",
    "\n",
    "- Ridge: L2-regularized linear regression\n",
    "- Lasso: L1-regularized linear regression\n",
    "\"\"\"\n",
    "ridge = Ridge(alpha=1.0)\n",
    "lasso = Lasso(alpha=0.1)\n",
    "\n",
    "\"\"\"\n",
    "Create and train a stacking model using Ridge as the final estimator.\n",
    "\n",
    "- estimators: base models (XGB + CatBoost)\n",
    "- final_estimator: Ridge Regression\n",
    "- n_jobs=-1: utilizes all CPU cores for parallel processing\n",
    "\"\"\"\n",
    "stack_ridge = StackingRegressor(estimators=base_models, final_estimator=ridge, n_jobs=-1)\n",
    "stack_ridge.fit(X_train, y_train)\n",
    "\n",
    "\"\"\"\n",
    "Generate predictions using the Ridge-based stacked model.\n",
    "\n",
    "- y_pred_ridge: predictions on the test set\n",
    "\"\"\"\n",
    "y_pred_ridge = stack_ridge.predict(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Create and train a stacking model using Lasso as the final estimator.\n",
    "\n",
    "- final_estimator: Lasso Regression (L1 regularization)\n",
    "\"\"\"\n",
    "stack_lasso = StackingRegressor(estimators=base_models, final_estimator=lasso, n_jobs=-1)\n",
    "stack_lasso.fit(X_train, y_train)\n",
    "\n",
    "\"\"\"\n",
    "Generate predictions using the Lasso-based stacked model.\n",
    "\n",
    "- y_pred_lasso: predictions on the test set\n",
    "\"\"\"\n",
    "y_pred_lasso = stack_lasso.predict(X_test)\n",
    "\n",
    "def evaluate_model(name, y_pred):\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = mse ** 0.5 \n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100  # handles MAPE calculation\n",
    "\n",
    "    print(f\"\\n{name} Final Estimator:\")\n",
    "    print(f\"R¬≤ Score: {r2:.4f}\")\n",
    "    print(f\"RMSE: {rmse:.2f}\")\n",
    "    print(f\"MAE: {mae:.2f}\")\n",
    "    print(f\"MAPE: {mape:.2f}%\")\n",
    "\n",
    "\"\"\"\n",
    "Evaluate and compare both stacking models (Ridge vs. Lasso).\n",
    "\"\"\"\n",
    "evaluate_model(\"üìò Ridge\", y_pred_ridge)\n",
    "evaluate_model(\"üìô Lasso\", y_pred_lasso)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
